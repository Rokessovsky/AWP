# -*- coding: utf-8 -*-
"""Segementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ln3YIKf_g2BremnAAjfTFtLZ_RDP2Pg
"""

# Copyright (c) Meta Platforms, Inc. and affiliates.

"""# Object masks from prompts with SAM

The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt. 

The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction.
"""

from IPython.display import display, HTML
display(HTML(
    """
    <a target="_blank" href="https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb">
      <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    </a>
    """
))

"""## Environment Set-up

If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'.
"""

using_colab = True

if using_colab:
    import torch
    import torchvision
    print("PyTorch version:", torch.__version__)
    print("Torchvision version:", torchvision.__version__)
    print("CUDA is available:", torch.cuda.is_available())
    import sys
    !{sys.executable} -m pip install opencv-python matplotlib
    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'
    !{sys.executable} -m pip install ultralytics

    !mkdir images
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg
    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/groceries.jpg

    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth

"""## Set-up

Necessary imports and helper functions for displaying points, boxes, and masks.
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
from google.colab import drive
import glob
import os

"""## Image pre-processing"""

from google.colab import drive
drive.mount('/content/drive')

yolo_data = "/content/drive/MyDrive/cows_datasets"
!zip -r /content/drive/MyDrive/cows_data.zip /content/drive/MyDrive/cows_datasets

"""Extracting frames from videos"""

# Path to the parent directory
path = '/content/drive/MyDrive/videos'
frames_path = '/content/drive/MyDrive/frames'

# Make sure the frame directory exists
os.makedirs(frames_path, exist_ok=True)

# List to store video names
video_names = []

# Global variables for frame extraction
NUM_FRAMES = 15  # number of frames to extract from each video
TIME_INTERVAL = 0.1  # time interval in seconds between each extracted frame

# Loop through all directories and subdirectories
for subdir, dirs, files in os.walk(path):
    # Check if directory is not empty
    if files:
        # Get the last part of directory which is considered as the video number
        video_number = os.path.basename(subdir)
        # Sort files to ensure naming is in order
        files.sort()
        # Enumerate files with 1-based index and construct name
        for index, file in enumerate(files, start=1):
            video_name = f"{video_number}_{index}_mp4"
            video_names.append(video_name)

            # Load the video using OpenCV
            video_path = os.path.join(subdir, file)
            vidcap = cv2.VideoCapture(video_path)

            # Get video frames per second (fps)
            fps = vidcap.get(cv2.CAP_PROP_FPS)
            # Calculate the frame skip based on desired time interval
            frame_skip = int(fps * TIME_INTERVAL)

            # Get total frames
            total_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)
            # Calculate the start and end frame for the middle few 5%
            start_frame = int(total_frames * 0.45)
            end_frame = int(total_frames * 0.50)

            success, image = vidcap.read()
            frame_count = 0
            extracted_frames = 0
            while success:
                # Check if this frame is one of the frames we want to extract
                if frame_count % frame_skip == 0 and extracted_frames < NUM_FRAMES and start_frame <= frame_count <= end_frame:
                    frame_name = f"{video_name}_{extracted_frames + 1}.png"
                    frame_path = os.path.join(frames_path, frame_name)
                    cv2.imwrite(frame_path, image)

                    # Add the following code to display the image:
                    img = mpimg.imread(frame_path)
                    imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                    plt.show()

                    extracted_frames += 1
                success, image = vidcap.read()
                frame_count += 1

"""## Segementation"""

def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor

sam_checkpoint = "sam_vit_h_4b8939.pth"
model_type = "vit_h"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

import os

train_path = '/content/drive/MyDrive/train_imgs'
test_path = '/content/drive/MyDrive/test_imgs'

# Make sure the frame directory exists
os.makedirs(train_path, exist_ok=True)
os.makedirs(test_path, exist_ok=True)

def process_image(image_path, train_path, test_path, test_ratio=0.05):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    predictor.set_image(image)

    input_point = np.array([[1000, 630], [950, 570], [1060, 600]])
    input_label = np.array([1, 1, 1])

    masks, scores, logits = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=True,
    )

    mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask

    masks, _, _ = predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        mask_input=mask_input[None, :, :],
        multimask_output=False,
    )

    masks.shape

    plt.figure(figsize=(10,10))
    plt.imshow(image)
    show_mask(masks, plt.gca())
    show_points(input_point, input_label, plt.gca())
    plt.axis('off')
    plt.show()

    # Apply the mask to the image
    masked_image = image * masks[0][:, :, None]  # If masks has more than 1 dimension, select the relevant one

    # Convert the masked image back to BGR color scheme for saving
    masked_image = cv2.cvtColor(masked_image.astype(np.uint8), cv2.COLOR_RGB2BGR)

    # Extract label from file name
    label = os.path.basename(image_path).split('_')[0]

    # Randomly assign an image to test set based on the test_ratio
    if np.random.rand() < test_ratio:
        new_dir = os.path.join(test_path, label)
    else:
        new_dir = os.path.join(train_path, label)

    # If the destination subdirectory doesn't exist, create it
    if not os.path.exists(new_dir):
        os.makedirs(new_dir)

    # Save the masked image to specific subdirectory
    cv2.imwrite(os.path.join(new_dir, f'mask_{os.path.basename(image_path)}'), masked_image)

# The directory where the frames are stored
frame_dir = '/content/drive/MyDrive/frames'


# Process each image in the frame directory
for image_file in os.listdir(frame_dir):
    image_path = os.path.join(frame_dir, image_file)
    if os.path.isfile(image_path):
        process_image(image_path, train_path, test_path)

"""## Fine-tuning the Model

prepare the data
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_path = '/content/drive/MyDrive/train_imgs'
test_path = '/content/drive/MyDrive/test_imgs'

# Define the generators for training and testing data
datagen = ImageDataGenerator(rescale = 1./255,
                             shear_range = 0.2,
                             zoom_range = 0.2,
                             horizontal_flip = True)

train_generator = datagen.flow_from_directory(train_path,
                                              target_size = (600, 600),
                                              batch_size = 16,
                                              class_mode = 'categorical')

test_generator = datagen.flow_from_directory(test_path,
                                             target_size = (600, 600),
                                             batch_size = 16,
                                             class_mode = 'categorical')

from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras import layers

# Load the pre-trained model
base_model = EfficientNetB7(weights='imagenet', include_top=False, pooling='avg')

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Unfreeze the last 20 layers
for layer in base_model.layers[-20:]:
    if not isinstance(layer, layers.BatchNormalization):
        layer.trainable = True

# Add a new dense layer for the output
output = Dense(train_generator.num_classes, activation='softmax')(base_model.output)

# Define the new model
model = Model(inputs=base_model.inputs, outputs=output)

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_generator, validation_data=test_generator, epochs=20)

# Save the model
model.save('finetuned_model.h5')

"""YOLO"""

!pip install ultralytics
from ultralytics import YOLO

yolo_data = "/content/drive/MyDrive/cows_datasets"
# Load a pretrained YOLO model (recommended for training)
yolo = YOLO('yolov8n-cls.pt')

# Train the model
yolo.train(data=yolo_data, epochs=50, imgsz=640)

# Validate the model
metrics = yolo.val()  # no arguments needed, dataset and settings remembered
print(metrics.top1)   # top1 accuracy
print(metrics.top5)   # top5 accuracy

test_path = "/content/drive/MyDrive/cows_datasets/test/6062"
result_path = "/content/drive/MyDrive/predict_results"

# Loop through the images in the folder
for image_file in os.listdir(test_path):
    if image_file.endswith('.jpg') or image_file.endswith('.jpeg') or image_file.endswith('.png'):
        image_path = os.path.join(test_path, image_file)
        result = yolo(image_path)
        res_plotted = result[0].plot()

        # Generate a save path with a new name
        save_path = os.path.join(result_path, "result_" + image_file)
        cv2.imwrite(save_path, res_plotted)
        cv2.imshow("result", res_plotted)

"""Clustering"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# for loading/processing the images
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array
from keras.applications.vgg16 import preprocess_input
import cv2

# models
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.applications import EfficientNetB7

# clustering and dimension reduction
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans

# for everything else
import os
import numpy as np
import matplotlib.pyplot as plt
from random import randint
import pandas as pd
import pickle
from sklearn.neighbors import NearestNeighbors

# Create a new model by removing the last layer from the fine-tuned model
feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)

# Extract features from the test dataset
features = feature_extractor.predict(test_generator)

def view_cluster(cluster):
    # Get filenames from the generator
    filenames = test_generator.filenames

    # Get the indices of files in this cluster
    indices = np.where(hclust.labels_ == cluster)[0]

    # Limit to 30 images
    if len(indices) > 30:
        print(f"Clipping cluster size from {len(indices)} to 30")
        indices = indices[:30]

    plt.figure(figsize = (15, 10))

    # Loop over the images of this cluster
    for i, index in enumerate(indices):
        img = load_img(os.path.join(test_path, filenames[index]))
        plt.subplot(5, 6, i + 1)
        plt.imshow(img)
        plt.axis('off')

    plt.show()

# Reduce dimensionality
pca = PCA(n_components=12, random_state=22)
pca.fit(features)
features_pca = pca.transform(features)

# Perform hierachical clustering
hclust = AgglomerativeClustering(n_clusters=19)
hclust.fit(features)

# Loop through the clusters
for i in range(hclust.n_clusters_):
    print(f"Cluster {i}:")
    view_cluster(i)

# Perform KMeans clustering
kmeans = KMeans(n_clusters=19, random_state=22)
kmeans.fit(features_pca)

# Loop through the clusters
for i in range(kmeans.n_clusters):
    print(f"Cluster {i}:")
    view_cluster(i)